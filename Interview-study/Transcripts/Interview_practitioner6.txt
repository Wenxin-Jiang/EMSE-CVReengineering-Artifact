Speaker 1:
So first, I'd like to ask, have you signed the consent form?
Speaker 2:
Yes.
Speaker 1:
Okay. So this interview is related to the machinery re-engineering process. So here we have five sets of questions, the background, the process workflow, the peak force, effective practices in this process, and some other questions. So first, I would like to ask some warm-up questions for our interview in order to know more about your background in machine learning and software engineering.
So the background questions can be answered briefly so that we can save more time for the other interesting questions. So first, could you please estimate your machinery expertise level? You may use novice, intermediate, expert, or master.
Speaker 2:
I would say expert.
Speaker 1:
And could you please estimate your software engineering level?
Speaker 2:
Intermediate.
Speaker 1:
So how many years have you worked on machine learning professionally?
Speaker 2:
Around two years, professionally, and two years before that in the university.
Speaker 1:
And what about the software engineering?
Speaker 2:
In a general sense, I would say around the same period.
Speaker 1:
Okay. So could you tell me your educational background?
Speaker 2:
So I have a bachelor's degree in control and automation engineering, and I am currently a master's student in mechanical engineering.
Speaker 1:
So, where did you learn about machine learning and software engineering?
Speaker 2:
I learned about it in a laboratory in my university when I was getting my bachelor's degree. It's a laboratory specified in machine learning and computational intelligence in general. I started a project there, started doing research, and that was it.
Speaker 1:
So roughly, how many machine learning projects have you worked on?
Speaker 2:
Well, that's very hard to say. I would say around hundreds, not hundreds, tens. I don't know. A lot.
Speaker 1:
And how many software engineering projects have you worked on?
Speaker 2:
Probably the same amount.
Speaker 1:
So do you work in a team? Or you work individually?
Speaker 2:
In a team.
Speaker 1:
So how many people are there in your team?
Speaker 2:
People in general, I would say around 20 people, and out of those 20 people, maybe six data scientists. That's all.
Speaker 1:
And what is your role in your team?
Speaker 2:
I am a data science engineer.
Speaker 1:
So what kinds of models have you worked on?
Speaker 2:
Basically, a lot of industrial plants modeling for advanced control. Also, a couple of reinforcement learning projects for again, controlling those plants and some computer vision models as well.
Speaker 1:
So now please take another look at the definitions here and pacing about the projects where you, or your team, reengineer machinery model using research papers or existing implementations. So what are your goals of re-engineering such a model?
Speaker 2:
Sorry. Can you please repeat the last part of the question?
Speaker 1:
Yeah. And so think about the projects where you, or your team, re-engineer a machinery model using research papers or existing implementations. So what are your goals of re-engineering such a model?
Speaker 2:
Yeah. So that happens a lot when it comes to working with industrial processes, but I would say in a more particular way. So in computer vision projects that I've worked with, a lot of transfer learning happens. So you get a model for a more general task and retrain it. But more recently, when I'm working with industrial tasks, you can't really extrapolate that much and reproduce that much from previous works.
What we usually do is we get the family of models that are used in a paper or in a previous project and try to adapt that family of models into that specific context. So, for instance, if you can see a convolutional recurrent architecture, deep architecture for predicting for a soft sensor, for instance, you wouldn't get the exact same architecture for your industrial plant, but you get the same building blocks.
So you also use a convolutional neural network layer. You also use a recurrent network layer, but the scale is different. The hyper-parameters are different, and the architecture overall is usually different.
I would say using the concepts, the definitions that we have on the screen, I hardly ever do reproduction or reproducibility. I would say I am more into the adaptation side and trying to adapt the model, not only adapting to different data but to different contexts.
Speaker 1:
So do you find that is where you are trying to adapt to the model in different tasks? The context was similar enough for different kinds of works.
Speaker 2:
I hardly ever work in something that I can't find any paper that has at least tried some implementation of a machine learning model.
Speaker 1:
So the next set of questions is really to choose a machinery re-engineering process workflow. We are trying to understand the process that software engineers follows as they try to bring machinery research results into their own organization. So can you talk me through the process that your team follows to reengineer machinery model from research papers, existing implementations, or another engineer's project?
Speaker 2:
Yeah. So I guess it's pretty much what I've mentioned in the previous question. So we start by doing a general literature review and try to get the general sense of the current state-of-the-art implementations on that specific context. And sometimes, you also look back into our own projects and try to see if there are any previous models that we can get inspiration from.
And after that, we filter out some of the papers that are often not that replicable. So a lot of times, I'll say that I don't know if this is endemic to our machine learning field, but often I see papers that simply they do not give you enough information to try to adapt or reproduce. So they intentionally or not, they can seal critical information.
So we filter out those papers and try to stick with those that would allow for reproduction, even though we probably are not going to reproduct them as is, and start by trying to get inspiration from the architecture. So see those papers, get the idea of what are the building blocks are used.
Something that I would say, it's quite relevant, often we do not get that much from the data engineering part of the project. So I'll say that that's so specific to each context that I rarely get much from the feature engineering or filtering and all of those initial steps in the machine learning workflow. We go more to a model-driven. So not as much as a data-centric. So model-centric approach when reading those papers and try to see what are the family of models they're most successful.
Speaker 1:
So then, I'd like to show you a process workflow. Please take a look at this workflow. This here. There are closed the record for a minute, and there are several questions related to this workflow, so you can tell me when you are ready.
Speaker 2:
I got it.
Speaker 1:
Okay. So can you tell me if you think this is an accurate process workflow or if you would like to change anything here based on your team's actual process?
Speaker 2:
I would say it's pretty good. It may be a little ideal. I'm not sure if we follow all of those steps in every single project, but it's a good idea of what it should do probably.
Speaker 1:
So this workflow does not contempt. Does your engineering stage, would you like to add that into this diagram?
Speaker 2:
Yeah. I'd say it gets into the data. I interpret the data pipeline block as the one who could contain a feature engineering.
Speaker 1:
Okay. So based on this workflow or based on your team's actual process, how do you measure the success of your re-engineering process?
Speaker 2:
I would say that it's also very specific to the problem to the task at hand. So the overall performance of the model, not much in comparison to the original implementation. So what we actually got is what truly matters. So we just evaluate our model as if it was fully developed by ourselves most of the time.
Speaker 1:
So would you like to add any back edges, things this diagram?
Speaker 2:
Sorry. Add what?
Speaker 1:
Back edges, like arrows from the code review box to the low version box here?
Speaker 2:
Oh yeah. Yeah. There's often, sometimes when you're in a more advanced stage, you feedback some of the information that you got and restructure some of the previous steps. Sometimes I'll maybe get an arrow back from the model called optimization, because sometimes if you're really interested in optimizing your speed of implementation, you may give up some previous processing function. In the model code optimization, if you find out that your model is too slow, you may start over and try to do things differently.
Speaker 1:
So how does your team updates new iterations of your model if it does not work for the first time?
Speaker 2:
In a practical sense, we just use Git, versioning of the models, and if something's wrong, we roll back to the last version that was working.
Speaker 1:
So do you do some testing?
Speaker 2:
Yeah. So all the time. If we have a specific data set that we are working with, let us assume that it's a specific problem with a specific data set and not a class of problems. The first thing we do is we split the data in three ways. So training validation and test, and the test set is sacred, who never touch it until the end of everything. And every step that we take, we usually see how that would affect the validation training, sorry, the validation set.
Speaker 1:
So the next set of questions is related to the pitfalls in sharing re-engineering process. We are trying to find hot spots in this process. So as there any hot spots in the machinery re-engineering process, for which parts do you think are challenging when re-engineering your model?
Speaker 2:
I would say that understanding the previous work is often the most difficult part because if the model is complex enough, translating that information and transmitting that information in an efficient way, it can be very hard. And so, like I said, when we are doing the literature review, a lot of times, some papers look very interesting.
The authors claim that they got very interesting results, but if you can't really understand what they did, then it's worthless. So the most challenging part for me, myself, I would say, is actually understanding the implementation and the ideas that guided the previous work.
Speaker 1:
And can you describe any challenges met when you implement a loss function, a model in the pipeline?
Speaker 2:
So loss function specifically, you mean?
Speaker 1:
So I mean your version here, so this includes the pipeline model and loss function.
Speaker 2:
Yeah. So the loss function, particularly it's not something that I pay a lot of attention. So outside of some particular cases, for instance, if you need to penalize outliers, or if you need to penalize large mistakes in a regression problem, I can perhaps write a custom loss, but most of the time, I go with the state-of-the-art usual mean squared error, mean absolute error, whatever. I'm really paid that much attention into the loss function only if it's a problem. So it presents a problem for me. Then I start thinking about it.
Speaker 1:
And what about the changes in data pipeline?
Speaker 2:
The data pipeline is probably the part where I spend most of my time. I think it's the most relevant step. And there are many challenges, so it's hard to pick one. But I still feel that a lot of what we do is it's based on some heuristics and could still use a more scientific method, maybe. So, for instance, when we are doing the feature selection, often if we are thinking about the sort of multi-objective optimization of finding a model that has the least amount of features that are most representative of the whole thing.
So what I often do sometimes I just write a custom multi-objective optimization problem and try to find scores for the number of features and scores for the performance of the model. And I would say that's the most challenging part because it's not like there's a formula of things that you can do. When you're using a model, sometimes it's easier. I would say you just do your Grit search or random search or things like that. When it comes to data pipeline, it's more of an art than science, I would say.
Speaker 1:
And so how do you, or your team address these challenges?
Speaker 2:
We just try to be very clear about everything that we do and document it very well. So everyone needs to be understanding of what we do. So if someone in the team doesn't get what you're trying to do, we assume that the client, or if you're publishing a paper, someone else's not going to understand it either. We just leave it behind.
Speaker 1:
So what kind of documentation would be here for using?
Speaker 2:
Documentation, you mean that I would receive to do? Or [inaudible 00:16:12] documentation?
Speaker 1:
I mean, you received?
Speaker 2:
Yeah. So I really like when people are very clear about how they define their hyper-parameters. Something that I think that it's lacking a lot of the times when I get previous work is how did you define your hyper-parameters? Which heuristics did you use? What were the candidates that you tested? I would say that that goes to all of the moving parts around the project.
I would prefer to get at least a pseudo-code of what's your idea on these more like tricky, more artsy parts of the project. I think that people lose a lot of time often just explaining the process. So if you get a paper, for instance, on UAV, you always get like three paragraphs of people saying how UAVs are ubiquitous nowadays. And I know that, so just be more objective.
Speaker 1:
And what about your own documentations?
Speaker 2:
I guess I tried to do the same thing. So I try to be very clear about what I am doing and not leave anything behind and always ask for other people's opinion because communicating scientific knowledge I think it's very difficult, but it's very important. So I will always try to get the feedback from my teammates and just to be sure that even if they weren't a part of that specific part of the product if they can understand what I did or what I wanted to say.
Speaker 1:
So have you met any challenges when you try to integrate all the components together?
Speaker 2:
Let me see if I can find an example. The moving from the more proof of concept model that you do in your Python notebook and actually implementing in some embedded system in an industry or whatever. That transfer is often what I would say is a more challenging so integrating your data pipeline, assuming that you have data in the insight, a function block that gets data from as an input and a prediction or whatever as the output.
So plugging that into the actual thing, that's going to run it. The industry, the embedded system, whatever, that's often what I find it most challenging.
Speaker 1:
So now, based on your experience, I'm wondering if you can think of one or two challenges to the process, to the tools or the artifacts that would make this re-engineering process easier for you or your team? So pausing it for a moment and then let me know your thoughts.
Speaker 2:
Like how would I suggest?
Speaker 1:
Yeah.
Speaker 2:
Changes in the workflow that we make-
Speaker 1:
This can be your changes to the workflow or some existing tools some research artifacts.
Speaker 2:
Mm-hmm (affirmative). Maybe I'm being too idealistic, but if we could perhaps come up with a convention of how things are done. So, for instance, if we could find a way to get this workflow, or perhaps lists some requirements and create some sort of standard when you're publishing your work, or when you're doing your work in your company, that will be realized in the future.
So if you could perhaps find a list, like some sort of list of requirements, they can just checkboxes and say, "Okay. So I did this, I did that. I fulfilled these requirements." And then you could say, "Okay. So this is a project that when the next engineer comes and try to reproduce this or adapt this, he probably won't have a lot of difficulties, so it will make your life easier." And that can be, for instance, like this workflow. So if you have this workflow, and if you find a way to make it public and widespread, perhaps that will help people out.
Speaker 1:
That's a good idea. So the next set of questions is on the effective re-engineering practices. So we'd like to know what practices you and your team adopt in your re-engineering process. So first, have you ever used any pre-trained models?
Speaker 2:
Yeah.
Speaker 1:
So how do you assess the viability for using these pre-trained models?
Speaker 2:
So my specific story was in my final paper, in my graduation. I reused a model that was trained for facial detection, and I adopted it to emotion detection. So my idea was simply how did I evaluate? Again, the problems were different. And I think that this often is the case when you're trying to use transfer learning. Perhaps by definition, the problem is in some way different.
So again, I would say that the evaluation itself isn't really connected to the previous implementation. The evaluation that truly matters is in the context of your application. But if I were to measure how successful I in reusing that in my problem would be, it would probably be from some comparison with a similar baseline. So in this specific example, I use pre-trained weights that were trained for facial detection. I got this performance.
What if I use the exact same architecture with randomly initialized weights? How would my performance be? So that would be my way to assess whether or not my decision to reuse the previous work was a good one or not.
Speaker 1:
And also, while you are using the existing models or the implementations, so this includes the codes or the documentations, how do you decide this kind of implementations are trustworthy?
Speaker 2:
Documentation, straight up. A good documentation is very important. Currently, I'm going through this right now because I'm working with reinforcement learning. And when it comes to reinforcement, things are very new, so you don't get a lot of packages, and they're not documented. For instance, you go to Keras [inaudible 00:23:57] documentation right now, and you'd go to the main functions, there will be just a hashtag write me as a comment for your function.
So I don't want to work with that. What I did is I had to write my own code for that. So I really think that if we are reusing code, documentation is just key for reliability.
Speaker 1:
And also, when you are referring to a research paper, so what do you find is helpful or problematic thing with this paper?
Speaker 2:
Sorry. Like in a specific paper?
Speaker 1:
Yeah.
Speaker 2:
So if I can give you an example of a specific paper that was problematic, is that right?
Speaker 1:
Yeah. That'd be great.
Speaker 2:
Yeah. Okay. There's this one paper it's very famous. It's called DeepVO. It's for deep visual odometry. It has a lot of citations, a lot of good people. But it has a lot of mistakes. I simply could not reproduce it. And if you look for other people that have tried to reproduce it, so there was this I think it was a final project for a undergrad, or maybe it was a master thesis that one of their goals just to reproduce their work, this DeepVO work, especially because their claims are like observed.
They're claiming to get very good performance. And if you look at that thesis dissertation, whatever, it basically got like something so much worse. It's unbelievable. It's not like even the same algorithm. You can tell that there's something there's just not enough information to reproduce it. And I think it's very odd that it gets so many citations, and it gets a lot of love from the community when no one can reproduce it. I don't even get what's the point of publishing something like that.
Speaker 1:
So what factors do you think that leads to this kind of scenario?
Speaker 2:
That is a very good question. I wish I had an answer. It will be very easy to just say, "Okay. Just make the standards for publication higher." But I don't think that's really it. I think it's maybe because machine learning research is still relatively new, and when it comes to deep models, there are so many heuristics and moving parts and things that we still do not fully understand, maybe.
So a lot of papers, perhaps they are overly ambitious when dealing with those models, and they make claims that they can not back with their performance. And I think it may be something more of a cultural thing with our field that it's still under development. So we still are learning how to communicate this information.
Speaker 1:
Yeah. And also, is there any other things that you think are very helpful in your research paper?
Speaker 2:
I would say that publishing your code and your data is amazing. I understand that this is often not possible, but it's very helpful when it is possible.
Speaker 1:
So how does your team work together to make the re-engineering process more effective?
Speaker 2:
Sorry, what was the last word you said?
Speaker 1:
How does your team work together to make the re-engineering process more effective? How do you send the feedbacks and give iterations of the model?
Speaker 2:
We try to be very organized with everything that we do and just keep everyone at least aware of what's happening. So I understand that paralyzing the work is crucial when you're dealing with big projects, but I also believe that it's very important that everyone knows what everyone else is doing, at least in idea. So we use a lot of the Git tools.
So a lot of GitLab tool, such as writing a lot of issues and assigning responsibilities to the ideas that we have and just try to keep everything well-documented while the project is running and not just leave everything to the end, because you probably are going to forget everything, a lot of what you've been through. And it's just very annoying to just do all your documentation at once. So in terms of good practices in machine learning and project in general, not only for re-engineering, what we try to do is just maintain everything that we have in our Git as organized as possible, and everyone needs to understand what's happening.
Speaker 1:
So are there existing tools or some other technologies that you found valuable or problematic for the re-engineering process? For example, ONNX, some testing tools, some visualization tools?
Speaker 2:
Well, specific to the re-engineering process again, I don't think I can of any, but I think that there's a surgence of a lot of tools for versioning of machine learning models specific. So, for instance, MLflow, which are still very new and they have a lot of issues, lots of issues. But the idea is really good to just allow for a good keeping the logs and artifacts of everything that was done.
So if we were thinking about re-engineering inside a same company or a same lab, so if you're going to reengineer your previous work, that can be very useful to just have an understanding of what was the context of not only the architecture and the data. So you can also understand how the training was carried out. And that can be also very relevant when dealing with stochastic nature models, such as deep neural networks because maybe you get a result in the past that you cannot direct produce in the future.
So I really liked those visualization tools because they add a lot of transparency. So this is also something that can be very tricky when trying to reproduce models if they're stochastic. And maybe even if someone provided a aggregated metrics on average of your performance, it still may not be that reproducible. When you allow people to see what was going on in the background of your training, that can be very helpful.
Speaker 1:
So you just mentioned the stochastic nature of machinery models. So I'm wondering have you met any instability in your models?
Speaker 2:
Oh, yeah. All the time. Yeah. So it often goes unstable.
Speaker 1:
Where did you find the instability, or how did you find that? How did you find that balance in your models?
Speaker 2:
Usually, when it comes to instability, it's pretty easy. You just see the loss exploding. Well, maybe it's something that may be more interesting. So, of course, everyone can tell you're dealing with instability if your loss explodes, but I feel like sometimes people forget to look at your gradient, the norm of your gradient. So sometimes, maybe even your loss, maybe not that big, not that large, but your gradient is just blowing up, so you can probably be suspicious that something is happening. But most of the time, I just make the learning rates smaller or normalize the gradient if I hadn't already, and things get better.
Speaker 1:
So how do you determine the acceptable trade-off between the performance of the model and the cost of your team? So the performance here can be the accuracy, and the speed and the cost can be time and money.
Speaker 2:
That is very problem-specific, I would say. So I think that the most relevant thing is just actually before starting your project, you define your initial goals, and you make them coherent with the manpower that you have available. So if you're working for a client, for instance, you make it very clear in your project proposal how many people will be working on and what are your expectations.
So I guess it's a matter of more adapting to your specific scenario. Then just a more general rule of thumb when dealing with a trade-off. It's something that's very relevant and deserves attention, but I would say it's more problem-specific.
Speaker 1:
So here we have the last set of questions, which is the comparison between machine learning engineering process and other software engineering process. So we are trying to find a specific feature of machine learning engineering process. So first, in what ways do you think the re-engineering process scientific efforts and in what way it is an engineering efforts?
Speaker 2:
Well, again, I think that's actually a very good question. And I would say that it's interesting to try to see these processes has scientific, but most of the time, I would say that they are engineering projects. And I would say there's also a very clear difference between software engineering in general and software engineering in the context of machine learning. So I guess that's the answer.
Speaker 1:
So here we have another questions. So can you talk about the difference between the machinery engineering process and the other software engineering process?
Speaker 2:
Yeah. So I guess I predicted that one. So there are many differences, but I would say that when it comes to software engineering in general, you're not as heuristic in your implementations. And usually, there's more of a concern in more of an organized model performance, non-model specific.
You're more concerned about the complexity of your code, and you're more aware of object-oriented programming techniques, whatever. I feel like when you're dealing with software engineering, you're more in general. Maybe I should rephrase it.
I would say that perhaps when you're working in machine learning, software engineering, often you neglect the other aspects of software engineering. That can be very important. To give you an example, something that I'm trying to get people in my team to use is PEP 8 convention. P-E-P 8 convention for Python.
So what often happens is everyone has their own way of programming, machine learning, and people they wanted to learn machine learning. And so, they learned programming for machine learning. They weren't programmers before they went into machine learning. So they didn't really have the experiences and best practices.
So I would say the largest difference is that machine learning code in general, I would say like, if you were to make some sort of a broad statistic estimation of most of the machine learning code that's written right now in the world, I would say that it's more disorganized and it lacks more of a attention to convention and performance and things like that.
Speaker 1:
And what do you think are other different aspects of machinery re-engineering compared to a general machinery battery process?
Speaker 2:
I would say that when you're dealing with re-engineering, you have the potential to make your life a lot easier. So if you selected the appropriate previous work that you're using as an inspiration, if it's well documented, if it is clear where you want to do, it can be a lot faster to implement.
You don't really have to deal with a lot of the creative issues that are more natural when you're doing something from ground zero. So I would say that the approach is different in those two projects. When you're dealing with a re-engineering project, at least in your rearview mirror, you always have a picture of what the previous work was. And you use that to avoid doing unnecessary work that was already done.
As when you're dealing with machine learning in general, there's a lot more exploration. You don't really have that much of a north. You don't really have a specific path that you want to take. So I would say it's more of a potential because a lot of times people say that they're going to do a re-engineering, but they end up saying that they're going to reuse, and they just give up in the middle and say, "Okay, so I'm going to change this, this, this, and that." And it becomes a whole different thing, but the potential is there.
Speaker 1:
So what recommendations or suggestions would you like to provide to engineers who are experts in machine learning but not as in software engineering?
Speaker 2:
Use conventions. Try to make your code clear and objective and be aware of performance in general. If you have a large loop, you're doing your computations with pandas, switch to non-pandas, maybe it's as simple as that. And it goes a lot faster. So it's more of a matter of paying attention.
And with time, you kind of get the feeling and the experience, and things get easier. But I would say that my advice would be just be aware of things, be aware of your lack of knowledge in software engineering, ask for help and try to see what are the spots in your code that are the slowest, the more complex. So things like that.
Speaker 1:
And what suggestions would you like to provide to those who are experts in software engineering but not machine learning?
Speaker 2:
Do the math. So those people, they already have the necessary tools, but they often do not have the mathematical background and foundations. It's really easy if you're a software engineer, just get a high-level package for Python and just plug it in, use Keras, get a deeplab, and just say, "Okay, so I have this deep neural network and this and that, and not have any idea of what's going on."
So my advice is, do not neglect understanding the mathematical and statistics foundation of machine learning. Do not become a technician that's only using things, tools that you have no idea of what they are doing in the background because this is something that often we do in software engineering. You just use components, and you often don't understand what's in the inside, but I say that that's very harmful in machine learning specific because it leads to a lot of mistakes.
And silly mistakes I would say from people who were breaking statistical rules, something like that, simply from neglecting this very important, fundamental knowledge.
Speaker 1:
So here we have the last questions. So are there any questions you would like to know, or you would like to ask other engineers who is involved in such a re-engineering process? So we may include your questions in the future interviews.
Speaker 2:
Honestly, I think that I'll be very curious to see other engineer's answers to the questions that I was asked. A lot of those questions really made me think of how I carry on my work. And I would say that they were very intriguing in some sense. I can't really think of anything that I would add to them.
Speaker 1:
So I think that's all the questions for our interview. I may stop recording.

